import { Project } from '../types';

export const PROJECTS: Project[] = [
  // --- PROJECTS (External Links) ---
  {
    id: '1',
    type: 'project',
    refId: '#REC-001',
    title: 'Movie Recommendation System',
    description: 'CineMatch is a movie recommendation system designed to provide personalized movie suggestions. It builds on data from The Movie Database (TMDb) and MovieLens to deliver relevant recommendations.',
    content: '',
    tags: ['Python', 'Content-based filtering', 'Collaborative filtering'],
    imageUrl: 'https://picsum.photos/seed/cine/600/400?grayscale',
    externalLink: 'https://github.com',
    date: '2023.12.01',
    stats: { label: 'DATASET', value: 'TMDB' }
  },
  {
    id: '2',
    type: 'project',
    refId: '#AI-099',
    title: 'AI-Powered Burnout Coach',
    description: 'Equa is a comprehensive system for burnout detection that uses machine learning and a large language model (LLM) to provide a proactive approach to workplace wellness through analyzing key employee metrics.',
    content: '',
    tags: ['Python', 'PyTorch', 'Scikit-learn', 'LLM'],
    imageUrl: 'https://picsum.photos/seed/ai/600/401?grayscale',
    externalLink: 'https://github.com',
    date: '2023.11.15',
    stats: { label: 'MODEL', value: 'LLM' }
  },
  {
    id: '3',
    type: 'project',
    refId: '#NLP-BERT',
    title: 'Sentiment Analysis',
    description: 'A fine-grained sentiment analysis project exploring classical and modern approaches to classify subtle emotional cues. Implementations include SVMs, RNNs and logistic regression, as well as transformer-based models (BERT / DistilBERT).',
    content: '',
    tags: ['Python', 'PyTorch', 'BERT', 'DistilBERT'],
    imageUrl: 'https://picsum.photos/seed/nlp/600/402?grayscale',
    externalLink: 'https://github.com',
    date: '2023.10.10',
    stats: { label: 'ACCURACY', value: 'SOTA' }
  },
  {
    id: '4',
    type: 'project',
    refId: '#GAME-CPP',
    title: 'Pac-Man CLI',
    description: 'A Pac-Man inspired game with randomly generated maps, custom colors, and a leaderboard. Maze generation uses DFS, ghost behavior is driven by BFS, visuals are rendered using ASCII art and Unicode.',
    content: '',
    tags: ['C++', 'CLI', 'ASCII visuals', 'DFS', 'BFS'],
    imageUrl: 'https://picsum.photos/seed/game/600/403?grayscale',
    externalLink: 'https://github.com',
    date: '2023.09.05',
    stats: { label: 'LANG', value: 'C++' }
  },

  // --- TALKS (Internal Blogs) ---
  {
    "id": "7749",
    "type": "talk",
    "refId": "#RES-30",
    "title": "Resume Guide",
    "description": "A quick guide I made for first year students on how to write a decent resume.",
    "content": "<h1>Building Your First Resume</h1><br /><h2>1. Understanding the Purpose of Your Resume</h2><br /><br />A resume is basically a quick, neat way to show people what you’ve done and what you can do.  <br />It’s not your life story, it’s more like a highlight reel.<br /><br /><h3><b>Academic vs. industry resumes</b>  </h3><br />If you’re aiming for a <b>TA<i> or </i>research assistant</b> position, the focus is on education, academic projects, teaching experience, skills, awards, and other achievements from your studies.  <br />For industry jobs, the spotlight is more on <b>skills<i>, experience, </i>measurable</b> results, and short project descriptions.<br /><br />For most undergrads, keeping it to one page is best. Two pages is fine only if you’ve actually done a lot worth putting in there not just because you want to fit in your high school debate trophy.<br /><h2>2. Core Principles for First-Time Resume Writers</h2><br /><br /><h3><b>ATS-friendly design</b>  </h3><br />- <a href='https://en.wikipedia.org/wiki/Applicant<i>tracking</i>system' target='_blank' rel='noopener noreferrer'>ATS (Applicant Tracking Systems)</a>it's basically a robot that scans resumes. <b>Robots get confused by fancy stuff</b>, so keep it simple. <br />- Pick standard fonts like Times New Roman, Arial, Calibri, or Helvetica.  <br />- If you’re using LaTeX, <code class=\"bg-white/10 px-1 rounded\">\\usepackage{lmodern}</code> is a safe choice.  <br />- Avoid tables for your main content; sections and bullet points are your friends.<br /><h3><b>Clarity and brevity</b>  </h3><br />- Use bullet points that start with action verbs like Designed, Developed, Led, or Analyzed.  <br />- Don’t write a paragraph in a bullet point keep it under two lines.<br /><h3><b>Measurable achievements</b>  </h3><br />- Numbers make things stronger. Instead of saying “Helped with grading,” say “Graded assignments for 60 students, with feedback ready in three days.”<br /><h3><b>Consistency</b>  </h3><br />- Dates should follow the same format throughout (e.g., Jan 2025 – May 2025).  <br />- Section titles should look the same in terms of font and size. Same with bullet styles.<br /><h3><b>Relevance</b>  </h3><br />- Cut anything that doesn’t help your case for the role.  <br />- And seriously make different versions for different roles. One-size-fits-all resumes just look lazy. Read the requirements and put in the things that match.<br /><br /><h2>3. Recommended Resume Structure</h2><br /><br />1. <b>Header</b>  <br />    Name, email, phone, LinkedIn, GitHub. No photo unless it’s required.<br />2. <b>Education</b>  <br />    University name, major, GPA if it’s decent (above 3.2/4.0), relevant coursework.<br />3. <b>Teaching Experience (or Academic Experience)</b>  <br />    TA roles, tutoring, mentoring. Use bullet points with numbers and specifics.<br />4. <b>Projects</b>  <br />    University or personal projects, especially if they’re related to teaching or open-source work.<br />5. <b>Skills</b>  <br />    Programming languages, tools, LaTeX, teaching-related software like GitHub Classroom.<br />6. <b>Awards and Achievements</b>  <br />    Scholarships, competitions, publications.<br />7. <b>Optional</b>  <br />    Languages or certifications.<br /><br /><h2>4. LaTeX Resume Building Tips</h2><br /><br />- Use Overleaf for LaTeX editing - <a href='https://www.overleaf.com/gallery/tagged/cv' target='_blank' rel='noopener noreferrer'>Overleaf Resume Templates</a><br />- <b>BUT<i> overleaf is filtered in Iran and it doesn't play nicely with vpns so I recommend to setup </i>TexStudio</b> but it is not that easy so the choice is yours.<br /><br />Good starting templates:<br />- <a href='https://github.com/posquit0/Awesome-CV' target='_blank' rel='noopener noreferrer'>Awesome CV</a> - modern and works well for academic stuff.<br />- <a href='https://www.overleaf.com/latex/templates/jakes-resume/syzfjbzwjncs' target='_blank' rel='noopener noreferrer'>Jake’s Resume</a> - clean and simple (My resume template) .<br /><br />Keep color use minimal <b>black<i> or </i>gray</b> is safest.  <br />Avoid text boxes, images, or complicated tables if you want your resume to play nice with ATS.<br /><br /><h2>5. Writing the Content</h2><br /><br />- Follow this formula for bullet points: <b>Action verb + task + result</b>.  <br />\t<b>Example</b>: \"Led a study group of 15 students, improving average scores by 20%\".<br />- Use past tense for past work and present tense for ongoing work.  <br />- For TA roles, highlight <b>the impact you had on students</b>.<br /><h2>6. What to Avoid</h2><br /><br />- <b>Two-column layouts</b>. You’re not making a magazine spread, also they are less ATS friendly.<br />- <b>Personal pronouns</b> like “I” or “my.”<br />- Don’t write <b>“helped”<i> or </i>“worked on.”</b> Be clear about what you did.  <br /><h2>7. Useful Resume-Building Websites</h2><br /><br />- <b>Templates</b><br />\t- <a href='https://www.overleaf.com/gallery/tagged/cv' target='_blank' rel='noopener noreferrer'>Overleaf CV Library</a><br />\t- <a href='https://www.latextemplates.com/cat/curricula-vitae' target='_blank' rel='noopener noreferrer'>LaTeX Templates – CVs</a><br />- <b>Industry Resume Checkers</b><br />\t- <a href='https://www.jobscan.co/' target='_blank' rel='noopener noreferrer'>Jobscan ATS Checker</a><br />\t- <a href='https://resumeworded.com/' target='_blank' rel='noopener noreferrer'>ResumeWorded</a><br />- <b>Academic Resume Examples</b><br />\t- <a href='https://capd.mit.edu/resources/resumes-cvs-cover-letters-and-linkedin/' target='_blank' rel='noopener noreferrer'>MIT Career Advising CV Guide</a><br /><h2>8. Adapting for Jobs Beyond TA Roles</h2><br /><br />- If you’re applying for industry jobs, focus more on <b>skills<i> and </i>results</b> rather than academic awards.  <br />- Add <b>soft skills</b> like teamwork, communication, or leadership.  <br />- You can also start with a short summary or objective at the top to explain what you’re looking for.<br /><h2>9. FAQ</h2><br /><br /><h3>1. Why use LaTeX instead of Word?  </h3><br />- LaTeX gives you more control over formatting and makes things look more professional.  <br />- It’s also a skill you’ll need in later semesters for technical reports in courses like <b>data Science<i> or </i>Machine Learning</b>.  <br />- If you’re in a rush, there are online resume builders, but I do not recommend you to use them.<br /><h3>2. Why make a resume now if I have nothing to put on it?  </h3><br />- Because you’ll need one eventually, and <b>it’s easier to start early</b> and just keep it updated.  <br />- When you’re in your third year and suddenly need a resume, <b>you won’t have to dig through old messages</b> or remember what year you joined that student club.  <br />- It’s also practice for writing about yourself in a professional way.<br /><h3>3. Should I include high school stuff?  </h3><br />Only <b>if it’s really relevant.</b> Winning a national coding competition in high school? Sure. Your 10th-grade math club attendance? Probably not.<br /><h3>4. Do I need a photo?  </h3><br />Not unless the application asks for it. <b>Most resumes are without photos</b> and they are just fine.<br /><h3>5. How often should I update my resume?  </h3><br /><b>Every semester</b> is a good habit. Add new projects, jobs, or awards while they’re fresh in your mind.<br /><h3>6. Can I use colors or fancy fonts to make my resume stand out?  </h3><br />You can, but <b>don’t overdo it</b>. A clean, readable layout will “stand out” more than bright purple headings.<br /><h3>7. Should I put hobbies on my resume?  </h3><br />Only if they’re interesting or connected to the role. “I play chess competitively” is fine. “I like watching movies” is filler.",
    "tags": [
      "Resume"
    ],
    "imageUrl": "https://cdn.enhancv.com/images/1920/i/aHR0cHM6Ly9jZG4uZW5oYW5jdi5jb20vSG93X3RvX3dyaXRlX2FfcmVzdW1lX2ZlYXR1cmVfaW1nXzdiMDFhYjNmNDEuanBn.jpg",
    "date": "2026-02-11",
    "stats": {
      "label": "READ_TIME",
      "value": "15 MIN"
    }
  }
,
  {
  "id": "8218",
  "type": "talk",
  "refId": "#SNN-397",
  "title": "Spiking Neural Networks",
  "description": "Is SNN the next \"Big Thing\" in AI?",
  "content": "I have long been interested in how to build intelligent machines; intelligence at the level of humans - what we now call Artificial General Intelligence (AGI). Since the late 1970s I have been on a quest to solve this problem. I have researched the most interesting techniques of the day, from Eliza to Expert Systems, to GOFAI (good old fashioned AI) to Artificial Neural Networks (ANNs) but they all seemed to lead to dead ends as a means to this end, despite many being of some utility within narrow domains. None were able to reach the level of generality that humans possess. As I studied ANNs beginning in the mid 1980s they seemed to offer a promising approach. ANNs resembled, at a very abstract level, how biological neural networks were constructed - nodes connected by weighted edges that could be strengthened or weakened based on training. ANNs are the basis of today's Deep Learning systems - essentially multi-layered neural networks. Once computing power became available some 25 years or so ago, we began to see significant advances in areas of speech recognition, computer vision, handwriting recognition, and many other domains. Most recently, the appearance of Large Language Models (LLMs) have led to unprecedented advances which are coming at a faster and faster pace, leading many to believe the fundamentals employed in these models may be the path to AGI. I count myself among those, but there are some significant challenges that will need to be addressed before that can happen - challenges a discussion of which will need to wait for another article. However, some of the significant problems with today's LLMs is the amount of computational power required to train them and their static nature, requiring months of offline training to update them with new information. Some techniques are being explored to address these limitations (eg Retrieval Augmented Generation, Fine Tuning, attached memory, longer context windows, etc), but each has its own set of implementation challenges. We need a better way.<br /><br />When I began studying ANNs, I became fascinated with how biological brains do what they do, and somewhat unhappy with the \"unnatural\" way ANNs were constructed. I understood that they were mathematical abstractions largely based on linear algebra designed to capture some aspects of what biological neural networks do, but they left enough detail out that I was never completely comfortable. I therefore began a journey into the world of neural network simulations. These are systems which, to varying degrees of fidelity, try to capture what biological neural networks do, some down to the level of molecular interactions. While these are fascinating systems which are of great value in learning how biological systems work within the field neuroscience, due to the immense amount of computing power required they were not practical for building an AI. For instance, Henry Markram of the Human Brain Project noted in 2009, their current models required roughly the equivalent of a high end desktop computer just to simulate a single brain cell (neuron) at a fraction of realtime, and that did not even include simulating STDP which is the core mechanism by which neural networks \"learn\". I needed a better way.<br /><br />This is the point at which I began looking into Spiking Neural Networks. The following is a brief discussion of what these systems are and how they may well be the future of AI.<br /><br /><h2>Spiking Neural Networks</h2><br /><br />Spiking Neural Networks (SNNs) stand as a transformative approach in the realm of artificial intelligence (AI), offering solutions to the pressing challenges faced by traditional AI models, including large language models (LLMs). Here we delve into the technical and conceptual advancements brought about by SNNs, highlighting their potential to revolutionize AI through energy efficiency, advanced hardware compatibility, temporal processing capabilities, and alignment with biological neural networks.<br /><br /><h2>Introduction to Spiking Neural Networks</h2><br /><br />SNNs are often referred to as the third generation of neural network models. They distinguish themselves from their predecessors by their ability to mimic the actual dynamics of biological neurons more closely. In a biological brain, neurons communicate via spikes - brief, discrete events in time - which are both energy-efficient and effective for processing information. SNNs simulate this process by using spikes as the fundamental unit of information and computation. This fundamental difference from traditional artificial neural networks (ANNs), which process continuous data, allows SNNs to operate in a more brain-like manner, leading to several significant advantages.<br /><br /><h2>Energy Efficiency and Sparsity</h2><br /><br />One of the most notable advantages of SNNs is their energy efficiency. In conventional ANNs, including LLMs, neurons are simulated as continuously active, leading to high computational and energy costs, especially for models with billions of parameters. SNNs, in contrast, utilize an event-driven mechanism where neurons only fire or 'spike' in response to specific stimuli. This sparsity of activity means that at any given time, only a small fraction of neurons are active, drastically reducing energy consumption.<br /><br />Technical studies estimate that SNNs can achieve up to two orders of magnitude in energy savings compared to traditional neural networks. This efficiency stems from their ability to exploit temporal sparsity - only computing or updating states when necessary, based on the incoming spike patterns. This characteristic not only makes SNNs more sustainable but also opens the door to deploying advanced AI models on low-power devices, such as smartphones and embedded systems, without compromising performance.<br /><br /><h2>Neuromorphic Hardware: Bridging the Gap</h2><br /><br />The synergy between SNNs and neuromorphic hardware is another frontier in AI research. Neuromorphic computing seeks to emulate the neural structure and functioning of the human brain in silicon, offering a hardware platform inherently suited to the event-driven nature of SNNs. Unlike traditional CPUs and GPUs, which are optimized for sequential processing of continuous data, neuromorphic chips are designed to handle the parallel, sparse, and asynchronous nature of spikes in SNNs.<br /><br />These chips utilize a network of artificial neurons and synapses that can process spikes directly, leading to significant improvements in speed and energy efficiency. For instance, Intel's Loihi and IBM's TrueNorth are examples of neuromorphic chips that demonstrate the potential for high-speed, low-power AI computations. By leveraging such hardware, SNNs can achieve real-time processing capabilities, making them ideal for applications requiring rapid decision-making, such as autonomous vehicles and real-time language translation.<br /><br /><h2>Temporal Dynamics: A Step Towards More Sophisticated AI</h2><br /><br />The inherent ability of SNNs to process temporal information gives them a distinct advantage in tasks that involve sequences or time-based data. This is important because biological brains do not process information in a static environment. We are constantly flooded with new information from our senses, as well as our viscera. Traditional ANNs, including those used in LLMs, often require additional mechanisms, such as recurrent neural networks (RNNs) or long short-term memory (LSTM) units, to handle temporal dependencies. SNNs, however, naturally incorporate time as an integral component of their computation, allowing them to model sequences and temporal patterns more efficiently.<br /><br />This temporal dimension enables SNNs to capture the dynamics of language, music, and other time-dependent phenomena with a higher degree of fidelity. For example, in natural language processing, this capability allows for a more nuanced understanding of context, tone, and progression over time, potentially leading to more coherent and contextually aware language generation in LLMs.<br /><br /><h2>Aligning Closer with Biological Processes</h2><br /><br />The design of SNNs is inspired to a much greater degree than traditional ANNs, by the workings of the human brain, offering a pathway to AI systems that not only compute more efficiently but also learn and adapt in more biologically plausible ways. This alignment with biological neural networks extends beyond mere efficiency; it encompasses the potential for AI systems to exhibit properties such as learning from fewer examples, generalizing across different tasks, and even demonstrating forms of intuition and creativity.<br /><br />In the context of learning, SNNs leverage mechanisms like spike-timing-dependent plasticity (STDP), a biologically inspired rule that adjusts the strength of connections between neurons based on the timing of their spikes. STDP allows SNNs to learn from temporal patterns in data, mimicking the way humans learn from experiences over time. This learning mechanism could lead to AI models that are more adaptable and capable of lifelong learning, gradually improving their performance as they are exposed to more data.<br /><br /><h2>Challenges and Future Directions</h2><br /><br />Despite their potential, SNNs face several challenges that need addressing to fully realize their capabilities. One significant hurdle is the complexity of training SNNs. Traditional training algorithms like backpropagation, widely used in ANNs, are less effective for the discrete, non-differentiable nature of spikes. Researchers are actively exploring alternative training methods, such as surrogate gradient approaches, to overcome this limitation. I have my own approach to this issue, which I hope to elucidate publish in the future once I have some concrete results to share.<br /><br />Another challenge is the current lack of widespread adoption of neuromorphic hardware, which is crucial for unlocking the full potential of SNNs. As the technology matures and becomes more accessible, it is expected that the compatibility between SNNs and neuromorphic chips will drive a new wave of AI applications, characterized by their efficiency, responsiveness, and closer resemblance to natural intelligence. Until that time, I have some approaches which drastically improve power efficiency within standard von Neumann architecture machines which again, I will share in some future discussion.<br /><br />It is also important to understand that SNNs provide only a substrate upon which the larger structure of the neural network must be built. We cannot expect to simply dump a large batch of spiking neurons into a bag and expect it to become intelligent. Rather, this substrate can be used to construct the many hundreds of function specific nuclei required to approach what our brains do. For example, just as we could not make decisions without the interactions between the prefrontal cortex and the basil ganglia, we cannot expect our AI system to do so without the SNN based equivalent of these regions. So for me, the first step is to construct this substrate, and then move onto structuring the connectome in such a way as to emulate what the biological nuclei do.<br /><br />Finally, it is my contention that SNNs represent a significant leap towards creating AI systems that are not only more efficient and sustainable but also capable of complex temporal processing and closer alignment with biological intelligence. By addressing the challenges associated with their training and deployment, SNNs have the potential to revolutionize AI, offering solutions to the energy and complexity issues plaguing current models. As research progresses, the integration of SNNs with neuromorphic hardware and the exploration of their unique capabilities will undoubtedly lead to the emergence of more advanced, efficient, and intelligent AI systems.",
  "tags": [
    "SNN",
    "AI",
    "LLM"
  ],
  "imageUrl": "https://www.iis.fraunhofer.de/en/ff/kom/ai/snn/jcr:content/socialMediaImage.img.4col.large.jpg/1744275147124/snn-keyvisual.jpg",
  "date": "2026-02-11"
}
];